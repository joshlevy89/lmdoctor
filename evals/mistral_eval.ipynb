{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "574c865c-c179-47cd-b331-9283352504d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86baf436-d4c3-49b5-8af4-82fb394c24ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compute_cdf(scores):\n",
    "    scores_sorted = np.sort(scores)\n",
    "    cdf = np.arange(1, len(scores) + 1) / len(scores)\n",
    "    return scores_sorted, cdf\n",
    "\n",
    "def plot_cdfs(data_list, names=None):\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for i in range(len(data_list)):\n",
    "        x, y = compute_cdf(data_list[i])\n",
    "        fig.add_trace(go.Scatter(x=x, y=y, mode='lines', name=names[i] if names is not None else None))\n",
    "    fig.update_layout(xaxis_title='Value', yaxis_title='CDF')\n",
    "    fig.show()\n",
    "\n",
    "def plot_scores_by_prompt(data, max_new_tokens=29):\n",
    "    \n",
    "    # Pad data\n",
    "    padded_data = [sublist + [np.nan] * (max_new_tokens - len(sublist)) for sublist in data]\n",
    "\n",
    "    # Create a heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=padded_data,\n",
    "        colorscale='RdYlGn',\n",
    "        zmin=0, \n",
    "        zmax=1,  \n",
    "        hoverongaps=False,  \n",
    "        showscale=True\n",
    "    ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        xaxis_title='Token Index',\n",
    "        yaxis_title='Prompt Index',\n",
    "        xaxis_nticks=max_new_tokens\n",
    "    )\n",
    "    \n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def get_scores_by_batch(doc, prompts, batch_size):\n",
    "\n",
    "    def _get_scores(doc, prompts, max_new_tokens=30):\n",
    "        \"\"\"\n",
    "        Run a batch of generation to get projections, and do detection on each.\n",
    "        \"\"\"\n",
    "        output = doc.generate(prompts, max_new_tokens=max_new_tokens, do_sample=False, gen_only=True, return_projections=True)\n",
    "        all_scores_per_token = []\n",
    "        for projs in output['projections']:\n",
    "            scores_per_token = doc.detect(projs, aggregation_method='auto')[0]\n",
    "            all_scores_per_token.append(list(scores_per_token))\n",
    "        return all_scores_per_token, output['text']\n",
    "    \n",
    "    all_scores = []\n",
    "    all_texts = []\n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        print(i)\n",
    "        these_scores, these_texts = _get_scores(doc, prompts[i:i+batch_size])\n",
    "        all_scores.extend(these_scores)\n",
    "        all_texts.extend(these_texts)\n",
    "    return all_scores, all_texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a31a213-f00f-462e-bc9d-861633f71082",
   "metadata": {},
   "source": [
    "## Setup\n",
    "If you already have a model/tokenizer you want to use, you can skip this step. \n",
    "Be sure to also set the appropriate user_tag/assistant_tag for that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be6ed83c-89d0-4d30-b794-20db2ebe92ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# The quantized model used here requires some extra libraries. \n",
    "import sys\n",
    "!{sys.executable} -m pip install torch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "!{sys.executable} -m pip install optimum>=1.12.0\n",
    "!{sys.executable} -m pip install auto-gptq==0.6.0\n",
    "!{sys.executable} -m pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a16924fa-784d-40d1-8ab6-651eb71b8c6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/cache/' # change or comment out as desired \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_model(model_name_or_path, revision, device):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path, device_map=device, revision=revision, trust_remote_code=False)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True, padding_side=\"left\")\n",
    "    tokenizer.pad_token_id = 0\n",
    "    return model, tokenizer\n",
    "\n",
    "device = 'cuda:0'\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
    "revision = 'gptq-4bit-32g-actorder_True'\n",
    "user_tag = \"[INST] \"\n",
    "assistant_tag = \" [/INST]\"\n",
    "\n",
    "model, tokenizer = load_model(model_name_or_path, revision, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b064e-1fe9-4679-b687-14be64371b47",
   "metadata": {},
   "source": [
    "## Train extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db112b6-7a23-4891-adfc-3eafb2135764",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lmdoctor.doctor import Doctor\n",
    "probe_type = 'pca'\n",
    "\n",
    "# default\n",
    "# extraction_target = 'honesty'\n",
    "# doc = Doctor(model, tokenizer, user_tag, assistant_tag, extraction_target=extraction_target, probe_type=probe_type, device=device)\n",
    "# doc.extract(n_train_pairs=128, n_dev_pairs=64, n_test_pairs=32)\n",
    "\n",
    "# for shuffle data experiment\n",
    "# extraction_target = 'honesty'\n",
    "# doc = Doctor(model, tokenizer, user_tag, assistant_tag, extraction_target=extraction_target, probe_type=probe_type, device=device)\n",
    "# doc.extract(n_train_pairs=128, n_dev_pairs=64, n_test_pairs=32, shuffle_functional_pairs=True)\n",
    "\n",
    "# for adding Creative Content to training set experiment\n",
    "extraction_target = 'honesty_plus'\n",
    "doc = Doctor(model, tokenizer, user_tag, assistant_tag, extraction_target=extraction_target, probe_type=probe_type, device=device)\n",
    "doc.extract(n_train_pairs=128, n_dev_pairs=64, n_test_pairs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b7555d-a2a8-4df9-9650-71d28f548ce1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc.statement_pairs['train'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfeb200-5350-4654-ab81-2b655b9232dc",
   "metadata": {},
   "source": [
    "## Lie Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c758d6-e5dc-4e1f-bdbb-9da373ad78a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "flp = pd.read_csv('./lie_requests/fact_lie_pairs.csv', header=None)\n",
    "flp.columns = ['prompt', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e00f0-e253-42f7-9bf4-bac216f9cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03a967f-c857-4b44-ad2c-6dc94a71fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = flp['prompt'].iloc[0]\n",
    "# prompt = flp['prompt'].iloc[3]\n",
    "# prompt = flp[flp['label']==1]['prompt'].tolist()[12]\n",
    "prompt = flp[flp['label']==0]['prompt'].tolist()[1]\n",
    "\n",
    "print(prompt)\n",
    "max_new_tokens = 30\n",
    "output = doc.generate(prompt, max_new_tokens=max_new_tokens, do_sample=False, gen_only=True, return_projections=True)\n",
    "output['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed36315-6750-4373-b8ec-a60d78e0208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.plot_projection_heatmap(output['projections'], output['tokens'], lastn_tokens_to_plot=max_new_tokens, saturate_at='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c480ad35-f51d-4cf5-88a6-cc3903965f8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_per_token = doc.detect(output['projections'], aggregation_method='auto')\n",
    "doc.plot_scores_per_token(scores_per_token, output['tokens'], lastn_tokens_to_plot=max_new_tokens, detection_method='classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d83c13b-6b10-4bf3-b058-d11ff3aff1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=5\n",
    "lie_prompts = flp[flp['label'] == 1]['prompt'].tolist()\n",
    "fact_prompts = flp[flp['label'] == 0]['prompt'].tolist()\n",
    "lie_scores, lie_texts = get_scores_by_batch(doc, lie_prompts, batch_size)\n",
    "fact_scores, fact_texts = get_scores_by_batch(doc, fact_prompts, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271783d8-5252-4853-9b0d-7dab69be5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "lie_scores_flat = np.concatenate(lie_scores)\n",
    "fact_scores_flat = np.concatenate(fact_scores)\n",
    "plot_cdfs((lie_scores_flat, fact_scores_flat), ['lie prompt', 'fact prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142be8be-c44a-4477-b434-c73c14e5f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores_by_prompt(lie_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a4f1e-1f3e-40fa-8cc2-1e8e34fdb536",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores_by_prompt(fact_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a082340-ac53-4c54-b5b3-31aa41977a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lie_agg = [np.mean(l) for l in lie_scores]\n",
    "fact_agg = [np.mean(l) for l in fact_scores]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=lie_agg, name='lie', opacity=0.75))\n",
    "fig.add_trace(go.Histogram(x=fact_agg, name='fact', opacity=0.75))\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1031e8-c054-48d2-8249-7d7d69b90a2e",
   "metadata": {},
   "source": [
    "## Unanswerable Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456e5d81-789d-4c29-a7bf-a67aa8f8f655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "qa = pd.read_csv('unanswerable_questions/hallucination_prompts.csv')\n",
    "qa.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dccb7b-179b-4a19-8bab-e024af5c6a7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = qa['Hallucination Question'].iloc[0]\n",
    "# prompt = qa['Factual Question'].iloc[0]\n",
    "\n",
    "print(prompt)\n",
    "max_new_tokens = 30\n",
    "output = doc.generate(prompt, max_new_tokens=max_new_tokens, do_sample=False, gen_only=True, return_projections=True)\n",
    "output['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc4abe8-0085-46a1-8694-69c0138d0421",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc.plot_projection_heatmap(output['projections'], output['tokens'], lastn_tokens_to_plot=None, saturate_at='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6916a9-baa0-4353-af44-d6ab11ed8f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores_per_token = doc.detect(output['projections'], aggregation_method='auto')\n",
    "doc.plot_scores_per_token(scores_per_token, output['tokens'], lastn_tokens_to_plot=max_new_tokens, detection_method='classifier')\n",
    "# doc.plot_scores_per_token(scores_per_token[:, 26:], output['tokens'][26:], lastn_tokens_to_plot=max_new_tokens, detection_method='classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec11bcb-2990-48e4-98b3-3e2d8c0f7f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 # for some reason, i see weird results in one batch when using batch of 32...probably due to an issue with the quantization (autogptq, model itself, exllama etc). Best to keep it to a low batch size.\n",
    "hallucination_questions = qa['Hallucination Question'].tolist()\n",
    "factual_questions = qa['Factual Question'].tolist()\n",
    "hallucination_scores, hallucination_texts = get_scores_by_batch(doc, hallucination_questions, batch_size)\n",
    "factual_scores, factual_texts = get_scores_by_batch(doc, factual_questions, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f0330c-ffb6-4555-90cf-3d5bc1277b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucination_scores_flat = np.concatenate(hallucination_scores) \n",
    "factual_scores_flat = np.concatenate(factual_scores) \n",
    "plot_cdfs((hallucination_scores_flat, factual_scores_flat), ['hallucination scores', 'factual_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4992ad52-ddcc-429d-b2ab-64a0167b9d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores_by_prompt(factual_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c412f12-4ca9-4380-9252-5cc84c600873",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores_by_prompt(hallucination_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f9f3d-dcde-4f0c-a5ad-6e56152cd6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucination_aggs = [np.mean(l) for l in hallucination_scores]\n",
    "factual_aggs = [np.mean(l) for l in factual_scores]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=hallucination_aggs, name='hallucination', opacity=0.75))\n",
    "fig.add_trace(go.Histogram(x=factual_aggs, name='factual', opacity=0.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea93f0f-9136-4a02-b13d-e2be3056c41c",
   "metadata": {},
   "source": [
    "## Creative Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1241ae-a9c0-4612-8e23-d3b5fc3aeeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_prompts = pd.read_csv('./creative_content/content_prompts.csv', header=None)\n",
    "content_prompts.columns = ['prompt', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2968e8d0-4ff9-4829-b082-a5e0cc2f1048",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_prompts.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f10993d-ddd6-4d4e-8ce9-762fe15db897",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = content_prompts['prompt'].iloc[20]\n",
    "\n",
    "print(prompt)\n",
    "max_new_tokens = 30\n",
    "output = doc.generate(prompt, max_new_tokens=max_new_tokens, do_sample=False, gen_only=True, return_projections=True)\n",
    "output['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba9def5-7dc9-4047-bde3-c887c635bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.plot_projection_heatmap(output['projections'], output['tokens'], lastn_tokens_to_plot=None, saturate_at='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa8f3b0-f688-4d48-8309-e15eb102ecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_per_token = doc.detect(output['projections'], aggregation_method='auto')\n",
    "doc.plot_scores_per_token(scores_per_token, output['tokens'], lastn_tokens_to_plot=max_new_tokens, detection_method='classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33be4a72-dd67-4c38-84fb-82209e1899e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "fictional_content_prompts = content_prompts[content_prompts['label'] == 1]['prompt'].tolist()\n",
    "factual_content_prompts = content_prompts[content_prompts['label'] == 0]['prompt'].tolist()\n",
    "fictional_content_scores, fictional_content_texts = get_scores_by_batch(doc, fictional_content_prompts, batch_size)\n",
    "factual_content_scores, factual_content_texts = get_scores_by_batch(doc, factual_content_prompts, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71911fbb-f1fe-4a0e-b9a3-b23a1677142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fictional_content_scores_flat = np.concatenate(fictional_content_scores)\n",
    "factual_content_scores_flat = np.concatenate(factual_content_scores)\n",
    "plot_cdfs((fictional_content_scores_flat, factual_content_scores_flat), ['fictional prompt', 'factual prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c8027e-b09b-4529-ae41-f662d675cfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores_by_prompt(fictional_content_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc3a796-ec5b-496a-8a53-9c343b43dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores_by_prompt(factual_content_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08d6a0c-84c4-4a98-8612-f8dab712c729",
   "metadata": {},
   "outputs": [],
   "source": [
    "fictional_content_scores = np.array(fictional_content_scores)\n",
    "factual_content_scores = np.array(factual_content_scores)\n",
    "plot_cdfs(([fictional_content_scores.mean(axis=1), factual_content_scores.mean(axis=1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbcb80c-9917-4bcc-a29a-b0bbf1e40837",
   "metadata": {},
   "outputs": [],
   "source": [
    "fictional_content_agg = [np.mean(l) for l in fictional_content_scores]\n",
    "factual_content_agg = [np.mean(l) for l in factual_content_scores]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=fictional_content_agg, name='fictional', opacity=0.75))\n",
    "fig.add_trace(go.Histogram(x=factual_content_agg, name='factual', opacity=0.75))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e9f3f-07b8-43f3-81ed-1c504a6e09a4",
   "metadata": {},
   "source": [
    "### Subjective Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85356b5c-a33a-46df-9007-16387ebe0030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opine = pd.read_csv('./opinions/opinions.csv')\n",
    "opine = pd.read_csv('./subjective_content/opinions_updated.csv')\n",
    "\n",
    "opine.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa96a7-66a3-40d5-b612-890b8a5a5fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "opine['Label'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d38f84-1792-44ca-9e86-5a84b0cffb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = opine[opine['Label'] == 'Subjective']['Prompt'].tolist()[1]\n",
    "prompt = opine[opine['Label'] == 'Ambiguous']['Prompt'].tolist()[2]\n",
    "# prompt = opine[opine['Label'] == 'Objective']['Prompt'].tolist()[10]\n",
    "\n",
    "print(prompt)\n",
    "max_new_tokens = 30\n",
    "output = doc.generate(prompt, max_new_tokens=max_new_tokens, do_sample=False, gen_only=True, return_projections=True)\n",
    "output['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35af5cb-c87a-4fec-b96b-9b1f076466dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.plot_projection_heatmap(output['projections'], output['tokens'], lastn_tokens_to_plot=max_new_tokens, saturate_at='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6f27c7-b0c6-4933-9558-407ab1734991",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_per_token = doc.detect(output['projections'], aggregation_method='auto')\n",
    "doc.plot_scores_per_token(scores_per_token, output['tokens'], lastn_tokens_to_plot=max_new_tokens, detection_method='classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e2af13-0476-4f51-8567-7ce260ee46fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "subjective_content_prompts = opine[opine['Label'] == 'Subjective']['Prompt'].tolist()\n",
    "ambiguous_content_prompts = opine[opine['Label'] == 'Ambiguous']['Prompt'].tolist()\n",
    "objective_content_prompts = opine[opine['Label'] == 'Objective']['Prompt'].tolist()\n",
    "subjective_content_scores, subjective_content_texts = get_scores_by_batch(doc, subjective_content_prompts, batch_size)\n",
    "ambiguous_content_scores, ambiguous_content_texts = get_scores_by_batch(doc, ambiguous_content_prompts, batch_size)\n",
    "objective_content_scores, objective_content_texts = get_scores_by_batch(doc, objective_content_prompts, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edd67bf-976f-4303-847c-f1e6992d985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores_by_prompt(subjective_content_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0810e89-c202-41cc-84eb-3b52895701cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores_by_prompt(ambiguous_content_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d96690-a31f-48ed-9781-c4ff45058538",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores_by_prompt(objective_content_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94419428-521a-4dea-9cee-2a09b716cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"Pretend you are a human who just ate a big meal. Question: Are you hungry?\"\n",
    "# prompt = \"Pretend you are a human who has not eaten all day. Question: Are you hungry?\"\n",
    "# prompt = \"[INST] Do you like dogs? [/INST] No, \"\n",
    "prompt = \"[INST] Do you like dogs? [/INST] Yes,\"\n",
    "\n",
    "should_format_prompt=False\n",
    "\n",
    "print(prompt)\n",
    "max_new_tokens = 30\n",
    "output = doc.generate(prompt, max_new_tokens=max_new_tokens, do_sample=False, gen_only=False, return_projections=True, should_format_prompt=should_format_prompt)\n",
    "output['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4952399-7122-499c-a39e-b82cc6846e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.plot_projection_heatmap(output['projections'], output['tokens'], lastn_tokens_to_plot=max_new_tokens, saturate_at='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa49ab-6d5d-4d8d-97d2-2316c5f48c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_per_token = doc.detect(output['projections'], aggregation_method='auto')\n",
    "doc.plot_scores_per_token(scores_per_token, output['tokens'], lastn_tokens_to_plot=max_new_tokens, detection_method='classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e68947-34c6-40e8-9c7d-fd4d241c291c",
   "metadata": {},
   "source": [
    "### Objective, Non-Factual Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3390095c-c5ce-4ae0-be3c-3d7255bd68a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"Please write a python function that prints 'do as i say' 10 times.\"\n",
    "prompt = \"Please write a python function that prints 'hello world' 10 times.\"\n",
    "\n",
    "print(prompt)\n",
    "max_new_tokens = 200\n",
    "output = doc.generate(prompt, max_new_tokens=max_new_tokens, do_sample=False, gen_only=True, return_projections=True)\n",
    "output['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef5a299-6800-4ca9-ba2f-7c97e9ecc050",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42f2004-a1ca-4774-a150-007eeee615b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on just the code part\n",
    "\n",
    "s = output['tokens'].index(':') + 1\n",
    "e = output['tokens'].index('()')\n",
    "output['tokens'][s:e]\n",
    "\n",
    "scores_per_token = doc.detect(output['projections'], aggregation_method='auto')\n",
    "doc.plot_scores_per_token(scores_per_token, output['tokens'], detection_method='classifier', token_ranges=[[s,e]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c59f53-d0c8-429e-858e-a805ec41413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"If I had 10 dollars and then spent 3, how many would I have left?\"\n",
    "# prompt = \"If I had 10 qubiots and then spent 3, how many would I have left?\"\n",
    "# prompt = \"If I had 10 gold-plated coins and then spent 3, how many would I have left?\"\n",
    "# prompt = \"If I had 10 triple-decker cheesburgers with extra mayo and ate 3, how many would I have left?\"\n",
    "\n",
    "# prompt = \"Consider the following situation: I fill a cup with water and mix in one scoop of powder ('original mixture'). I then pour out half of the mixture and mix in another scoop of powder ('final mixture'). How concentrated is the final mixture, relative to the original?\"\n",
    "prompt = \"Let's say I had a bag with 20 black marbles and 10 yellow marbles. I pull out a yellow marble and put it on the table. What are the chances I pull out another yellow marble on the second turn?\"\n",
    "\n",
    "print(prompt)\n",
    "max_new_tokens = 100\n",
    "output = doc.generate(prompt, max_new_tokens=max_new_tokens, do_sample=False, gen_only=True, return_projections=True)\n",
    "output['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5482a10e-3615-40bb-a86b-a2ab741bbd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.plot_projection_heatmap(output['projections'], output['tokens'], lastn_tokens_to_plot=None, saturate_at='auto')\n",
    "# doc.plot_projection_heatmap(output['projections'], None, lastn_tokens_to_plot=max_new_tokens, saturate_at='auto', aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0010a5fb-56b1-4b6a-a73a-30638982ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_per_token = doc.detect(output['projections'], aggregation_method='auto')\n",
    "# doc.plot_scores_per_token(scores_per_token, output['tokens'], lastn_tokens_to_plot=None, detection_method='classifier')\n",
    "doc.plot_scores_per_token(scores_per_token, None, lastn_tokens_to_plot=max_new_tokens, detection_method='classifier', figsize=(1000,200), aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a9fca-f602-4112-be2a-71b110d0027c",
   "metadata": {},
   "source": [
    "## Quality of probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5178fe-9717-4f83-be6f-cded87821317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the projections\n",
    "\n",
    "from lmdoctor import plot_utils, detection_utils\n",
    "proj_pairs = detection_utils.act_pairs_to_projs(doc.train_acts, doc.direction_info, len(doc.statement_pairs['train']))\n",
    "\n",
    "layer = 15\n",
    "projs_true = proj_pairs[0, :, layer]\n",
    "projs_lie = proj_pairs[1, :, layer]\n",
    "plot_utils.plot_projs_on_numberline(projs_true, projs_lie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6df8a83-7f1d-4271-be65-5a10cac9f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a scan from the training dataset\n",
    "\n",
    "input_text = doc.statement_pairs['train'][8][1]\n",
    "# input_text = doc.statement_pairs['train'][8][0] \n",
    "\n",
    "projections = doc.get_projections(input_text=input_text)\n",
    "tokens = tokenizer.tokenize(input_text)\n",
    "doc.plot_projection_heatmap(projections, tokens, saturate_at='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6997d5c8-c66e-46b0-98d9-bc12181e7b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
