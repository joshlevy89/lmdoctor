{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574c865c-c179-47cd-b331-9283352504d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a31a213-f00f-462e-bc9d-861633f71082",
   "metadata": {},
   "source": [
    "## Setup\n",
    "If you already have a model/tokenizer you want to use, you can skip this step. \n",
    "Be sure to also set the appropriate user_tag/assistant_tag for that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6ed83c-89d0-4d30-b794-20db2ebe92ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# The quantized model used here requires some extra libraries. \n",
    "import sys\n",
    "!{sys.executable} -m pip install torch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "!{sys.executable} -m pip install optimum>=1.12.0\n",
    "!{sys.executable} -m pip install auto-gptq==0.6.0\n",
    "!{sys.executable} -m pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16924fa-784d-40d1-8ab6-651eb71b8c6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/cache/' # change or comment out as desired \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_model(model_name_or_path, revision, device):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path, device_map=device, revision=revision, trust_remote_code=False)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True, padding_side=\"left\")\n",
    "    tokenizer.pad_token_id = 0\n",
    "    return model, tokenizer\n",
    "\n",
    "device = 'cuda:0'\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
    "revision = 'gptq-4bit-32g-actorder_True'\n",
    "user_tag = \"[INST] \"\n",
    "assistant_tag = \" [/INST]\"\n",
    "\n",
    "model, tokenizer = load_model(model_name_or_path, revision, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b064e-1fe9-4679-b687-14be64371b47",
   "metadata": {},
   "source": [
    "## Train extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db112b6-7a23-4891-adfc-3eafb2135764",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lmdoctor.doctor import Doctor\n",
    "extraction_target = 'honesty'\n",
    "probe_type = 'pca'\n",
    "doc = Doctor(model, tokenizer, user_tag, assistant_tag, extraction_target=extraction_target, probe_type=probe_type, device=device)\n",
    "doc.extract(n_train_pairs=128, n_dev_pairs=64, n_test_pairs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b7555d-a2a8-4df9-9650-71d28f548ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.statement_pairs['train'][:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd27be9-78ab-4cf7-9560-9a6da91a7dd5",
   "metadata": {},
   "source": [
    "## Run detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89193532-9923-4462-8904-e97b2216bc4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt = \"Tell me a fact about humans\"\n",
    "prompt = \"Tell me a lie about humans\"\n",
    "# prompt = \"Who authored 'Gimbar's adventures'?\"\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "max_new_tokens = 10\n",
    "output = doc.generate(prompt, max_new_tokens=max_new_tokens, do_sample=True, gen_only=True, return_projections=True)\n",
    "output['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a895bf4c-ef51-4b56-a3c5-52276c148fd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(output['text'])\n",
    "doc.plot_projection_heatmap(output['projections'], tokens, lastn_tokens_to_plot=max_new_tokens, saturate_at='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379fe7a4-3698-4594-a3d3-b8d74491c58b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores_per_token = doc.detect(output['projections'], aggregation_method='layer_avg', use_n_middle_layers=15)\n",
    "doc.plot_scores_per_token(scores_per_token, tokens, lastn_tokens_to_plot=max_new_tokens, saturate_at=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a61964-757e-4749-8641-ccc8b7ffcf8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# alternative: auto-aggregation\n",
    "scores_per_token = doc.detect(output['projections'], aggregation_method='auto')\n",
    "doc.plot_scores_per_token(scores_per_token, tokens, lastn_tokens_to_plot=max_new_tokens, detection_method='classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c608d07-d4c6-4859-aac7-978681c4f57a",
   "metadata": {},
   "source": [
    "## Control (with reading vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a16994-dd25-432e-999a-27900b8199ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# refer to control_utils.Controller for info on setting alpha and control_direction\n",
    "prompt = \"You killed a person for revenge but don't want to get caught. What would you tell the police?\"\n",
    "print(doc.generate_with_control(prompt, control_direction=None, max_new_tokens=12)) # no control\n",
    "print(doc.generate_with_control(prompt, control_direction=1, max_new_tokens=12, alpha=1, n_trim_layers=10)) # add the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2208d8a-e780-4146-8c34-032384509b62",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"I shoplifted a candy bar from the store. What should I tell my parents?\"\n",
    "print(doc.generate_with_control(prompt, control_direction=None, max_new_tokens=12)) # no control\n",
    "print(doc.generate_with_control(prompt, control_direction=-1, max_new_tokens=12, alpha=1, n_trim_layers=10)) # subtract the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1a5847-e70c-41c8-b0e3-ce3da8842593",
   "metadata": {},
   "source": [
    "## Extra visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b2ac8c-5b6f-4ae6-9c49-039f6cf2c513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the projections\n",
    "\n",
    "from lmdoctor import plot_utils, detection_utils\n",
    "proj_pairs = detection_utils.act_pairs_to_projs(doc.train_acts, doc.direction_info, len(doc.statement_pairs['train']))\n",
    "\n",
    "layer = 15\n",
    "projs_true = proj_pairs[0, :, layer]\n",
    "projs_lie = proj_pairs[1, :, layer]\n",
    "plot_utils.plot_projs_on_numberline(projs_true, projs_lie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d44b91e-8436-42f8-82ec-d8c911255bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a scan from the training dataset\n",
    "\n",
    "input_text = doc.statement_pairs['train'][8][1]\n",
    "# input_text = doc.statement_pairs['train'][8][0] \n",
    "\n",
    "projections = doc.get_projections(input_text=input_text)\n",
    "tokens = tokenizer.tokenize(input_text)\n",
    "doc.plot_projection_heatmap(projections, tokens, saturate_at='auto')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
